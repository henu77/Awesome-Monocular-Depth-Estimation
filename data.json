[
  {
    "year": "2025",
    "title": "Depth Any Camera: Zero-Shot Metric Depth Estimation from Any Camera",
    "venue": "CVPR",
    "abstract": "While recent depth foundation models exhibit strong zero-shot generalization, achieving accurate metric depth across diverse camera types-particularly those with large fields of view (FoV) such as fisheye and 360-degree cameras-remains a significant challenge. This paper presents Depth Any Camera (DAC), a powerful zero-shot metric depth estimation framework that extends a perspective-trained model to effectively handle cameras with varying FoVs. The framework is designed to ensure that all existing 3D data can be leveraged, regardless of the specific camera types used in new applications. Remarkably, DAC is trained exclusively on perspective images but generalizes seamlessly to fisheye and 360-degree cameras without the need for specialized training data. DAC employs Equi-Rectangular Projection (ERP) as a unified image representation, enabling consistent processing of images with diverse FoVs. Its core components include pitch-aware Image-to-ERP conversion with efficient online augmentation to simulate distorted ERP patches from undistorted inputs, FoV alignment operations to enable effective training across a wide range of FoVs, and multi-resolution data augmentation to further address resolution disparities between training and testing. DAC achieves state-of-the-art zero-shot metric depth estimation, improving \\delta_1 accuracy by up to 50% on multiple fisheye and 360-degree datasets compared to prior metric depth foundation models, demonstrating robust generalization across camera types.",
    "citation": "@InProceedings{guo2025depth,\n  title={Depth Any Camera: Zero-Shot Metric Depth Estimation from Any Camera},\n  author={Guo, Yuliang and Garg, Sparsh and Miangoleh, S Mahdi H and Huang, Xinyu and Ren, Liu},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2025}\n}",
    "code": "https://github.com/yuliangguo/depth_any_camera",
    "paper_url": "https://arxiv.org/pdf/2501.02464",
    "project": "https://yuliangguo.github.io/depth-any-camera/"
  },
  {
    "title": "Depth Pro: Sharp Monocular Metric Depth in Less Than a Second",
    "year": 2025,
    "venue": "ICLR",
    "paper_url": "https://arxiv.org/pdf/2410.02073",
    "code": "https://github.com/apple/ml-depth-pro",
    "supplementary": "",
    "project": "",
    "demo": "",
    "abstract": "We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions.",
    "citation": "@inproceedings{Bochkovskii2024:arxiv,\n  author     = {Aleksei Bochkovskii and Ama\"{e}l Delaunoy and Hugo Germain and Marcel Santos and\n               Yichao Zhou and Stephan R. Richter and Vladlen Koltun},\n  title      = {Depth Pro: Sharp Monocular Metric Depth in Less Than a Second},\n  booktitle  = {International Conference on Learning Representations},\n  year       = {2025}\n}"
  },
  {
    "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction",
    "year": 2025,
    "venue": "ICLR",
    "paper_url": "https://arxiv.org/pdf/2409.18124",
    "code": "https://github.com/EnVision-Research/Lotus",
    "supplementary": "",
    "project": "https://lotus3d.github.io/",
    "demo": "https://huggingface.co/spaces/haodongli/Lotus_Depth, https://huggingface.co/spaces/haodongli/Lotus_Normal",
    "abstract": "Leveraging the visual priors of pre-trained text-to-image diffusion models offers a promising solution to enhance zero-shot generalization in dense prediction tasks. However, existing methods often uncritically use the original diffusion formulation, which may not be optimal due to the fundamental differences between dense prediction and image generation. In this paper, we provide a systemic analysis of the diffusion formulation for the dense prediction, focusing on both quality and efficiency. And we find that the original parameterization type for image generation, which learns to predict noise, is harmful for dense prediction; the multi-step noising/denoising diffusion process is also unnecessary and challenging to optimize. Based on these insights, we introduce Lotus, a diffusion-based visual foundation model with a simple yet effective adaptation protocol for dense prediction. Specifically, Lotus is trained to directly predict annotations instead of noise, thereby avoiding harmful variance. We also reformulate the diffusion process into a single-step procedure, simplifying optimization and significantly boosting inference speed. Additionally, we introduce a novel tuning strategy called detail preserver, which achieves more accurate and fine-grained predictions. Without scaling up the training data or model capacity, Lotus achieves SoTA performance in zero-shot depth and normal estimation across various datasets. It also enhances efficiency, being significantly faster than most existing diffusion-based methods. Lotus' superior quality and efficiency also enable a wide range of practical applications, such as joint estimation, single/multi-view 3D reconstruction, etc.",
    "citation": "@inproceedings{li2024lotus,\n  title={Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction},\n  author={He, Jing and Li, Haodong and Yin, Wei and Liang, Yixun and Li, Leheng and Zhou, Kaiqiang and Liu, Hongbo and Liu, Bingbing and Chen, Ying-Cong},\n  booktitle={International Conference on Learning Representations},\n  year={2025},\n\n}"
  },
  {
    "title": "Scalable Autoregressive Monocular Depth Estimation",
    "year": 2025,
    "venue": "CVPR",
    "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Scalable_Autoregressive_Monocular_Depth_Estimation_CVPR_2025_paper.pdf",
    "code": "https://github.com/wjh892521292/DAR",
    "supplementary": "https://openaccess.thecvf.com/content/CVPR2025/supplemental/Wang_Scalable_Autoregressive_Monocular_CVPR_2025_supplemental.pdf",
    "project": "https://depth-ar.github.io/",
    "demo": "",
    "abstract": "This paper proposes a new autoregressive model as an effective and scalable monocular depth estimator. Our idea is simple: We tackle the monocular depth estimation (MDE) task with an autoregressive prediction paradigm, based on two core designs. First, our depth autoregressive model (DAR) treats the depth map of different resolutions as a set of tokens, and conducts the low-to-high resolution autoregressive objective with a patch-wise casual mask. Second, our DAR recursively discretizes the entire depth range into more compact intervals, and attains the coarse-to-fine granularity autoregressive objective in an ordinal-regression manner. By coupling these two autoregressive objectives, our DAR establishes new state-of-the-art (SOTA) on KITTI and NYU Depth v2 by clear margins. Further, our scalable approach allows us to scale the model up to 2.0B and achieve the best RMSE of 1.799 on the KITTI dataset (5% improvement) compared to 1.896 by the current SOTA (Depth Anything). DAR further showcases zero-shot generalization ability on unseen datasets. These results suggest that DAR yields superior performance with an autoregressive prediction paradigm, providing a promising approach to equip modern autoregressive large models (e.g., GPT-4o) with depth estimation capabilities. Project page: https://depth-ar.github.io/",
    "citation": "@InProceedings{Wang_2025_CVPR,\n    author    = {Wang, Jinhong and Liu, Jian and Tang, Dongqi and Wang, Weiqiang and Li, Wentong and Chen, Danny and Chen, Jintai and Wu, Jian},\n    title     = {Scalable Autoregressive Monocular Depth Estimation},\n    booktitle = {Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)},\n    month     = {June},\n    year      = {2025},\n    pages     = {6262-6272}\n}"
  },
  {
    "year": "2025",
    "title": "SharpDepth: Sharpening Metric Depth Predictions Using Diffusion Distillation",
    "venue": "CVPR",
    "abstract": "We propose SharpDepth, a novel approach to monocular metric depth estimation that combines the metric accuracy of discriminative depth estimation methods (e.g., Metric3D, UniDepth) with the fine-grained boundary sharpness typically achieved by generative methods (e.g., Marigold, Lotus). Traditional discriminative models trained on real-world data with sparse ground-truth depth can accurately predict metric depth but often produce over-smoothed or low-detail depth maps. Generative models, in contrast, are trained on synthetic data with dense ground truth, generating depth maps with sharp boundaries yet only providing relative depth with low accuracy. Our approach bridges these limitations by integrating metric accuracy with detailed boundary preservation, resulting in depth predictions that are both metrically precise and visually sharp. Our extensive zero-shot evaluations on standard depth estimation benchmarks confirm SharpDepth effectiveness, showing its ability to achieve both high depth accuracy and detailed representation, making it well-suited for applications requiring high-quality depth perception across diverse, real-world environments.",
    "citation": "@InProceedings{Pham_2025_CVPR,\n    author    = {Pham, Duc-Hai and Do, Tung and Nguyen, Phong and Hua, Binh-Son and Nguyen, Khoi and Nguyen, Rang},\n    title     = {SharpDepth: Sharpening Metric Depth Predictions Using Diffusion Distillation},\n    booktitle = {Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)},\n    month     = {June},\n    year      = {2025},\n    pages     = {17060-17069}\n}",
    "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Pham_SharpDepth_Sharpening_Metric_Depth_Predictions_Using_Diffusion_Distillation_CVPR_2025_paper.pdf",
    "project": "https://sharpdepth.github.io/",
    "supplementary": "https://openaccess.thecvf.com/content/CVPR2025/supplemental/Pham_SharpDepth_Sharpening_Metric_CVPR_2025_supplemental.pdf"
  },
  {
    "title": "ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation",
    "year": 2024,
    "venue": "CVPR",
    "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Patni_ECoDepth_Effective_Conditioning_of_Diffusion_Models_for_Monocular_Depth_Estimation_CVPR_2024_paper.pdf",
    "code": "https://github.com/aradhye2002/ecodepth",
    "supplementary": "https://openaccess.thecvf.com/content/CVPR2024/supplemental/Patni_ECoDepth_Effective_Conditioning_CVPR_2024_supplemental.pdf",
    "project": "https://ecodepth-iitd.github.io/",
    "demo": "",
    "abstract": "In the absence of parallax cues, a learning based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image. While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture. It has been shown that using embeddings from pretrained foundational models, such as CLIP, improves zero shot transfer in several applications. Taking inspiration from this, in our paper we explore the use of global image priors generated from a pre-trained ViT model to provide more detailed contextual information. We argue that the embedding vector from a ViT model, pre-trained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based text embeddings. Based on the idea, we propose a new SIDE model using a diffusion backbone conditioned on ViT embeddings. Our proposed design establishes a new state-of-the-art (SOTA) for SIDE on NYU Depth v2 dataset, achieving Abs Rel error of 0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on KITTI dataset, achieving SqRel error of 0.139 (2% improvement) compared to 0.142 by the current SOTA (GEDepth). For zero shot transfer with a model trained on NYU Depth v2, we report mean relative improvement of (20%, 23%, 81%, 25%) over NeWCRF on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%, 18%, 45%, 9%) by ZoEDepth.",
    "citation": "@InProceedings{Patni_2024_CVPR,\n    author    = {Patni, Suraj and Agarwal, Aradhye and Arora, Chetan},\n    title     = {ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    month     = {June},\n    year      = {2024},\n    pages     = {28285-28295}\n}"
  },
  {
    "title": "IEBins: Iterative Elastic Bins for Monocular Depth Estimation",
    "year": 2024,
    "venue": "NeurIPS",
    "paper_url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/a61023ce36d21010f1423304f8ec49af-Paper-Conference.pdf",
    "code": "https://github.com/ShuweiShao/IEBins",
    "supplementary": "https://proceedings.neurips.cc/paper_files/paper/2023/file/a61023ce36d21010f1423304f8ec49af-Supplemental-Conference.pdf",
    "project": "",
    "demo": "",
    "abstract": "Monocular depth estimation (MDE) is a fundamental topic of geometric computer vision and a core technique for many downstream applications. Recently, several methods reframe the MDE as a classification-regression problem where a linear combination of probabilistic distribution and bin centers is used to predict depth. In this paper, we propose a novel concept of iterative elastic bins (IEBins) for the classification-regression-based MDE. The proposed IEBins aims to search for high-quality depth by progressively optimizing the search range, which involves multiple stages and each stage performs a finer-grained depth search in the target bin on top of its previous stage. To alleviate the possible error accumulation during the iterative process, we utilize a novel elastic target bin to replace the original target bin, the width of which is adjusted elastically based on the depth uncertainty. Furthermore, we develop a dedicated framework composed of a feature extractor and an iterative optimizer that has powerful temporal context modeling capabilities benefiting from the GRU-based architecture. Extensive experiments on the KITTI, NYU-Depth-v2 and SUN RGB-D datasets demonstrate that the proposed method surpasses prior state-of-the-art competitors. The source code is publicly available at https://github.com/ShuweiShao/IEBins.",
    "citation": "@inproceedings{NEURIPS2023_a61023ce,\n  author = {Shao, Shuwei and Pei, Zhongcai and Wu, Xingming and Liu, Zhong and Chen, Weihai and Li, Zhengguo},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},\n  pages = {53025--53037},\n  publisher = {Curran Associates, Inc.},\n  title = {IEBins: Iterative Elastic Bins for Monocular Depth Estimation},\n volume = {36},\n  year = {2023}\n}"
  },
  {
    "title": "PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation",
    "year": 2024,
    "venue": "CVPR",
    "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_PatchFusion_An_End-to-End_Tile-Based_Framework_for_High-Resolution_Monocular_Metric_Depth_CVPR_2024_paper.pdf",
    "code": "https://zhyever.github.io/patchfusion/",
    "supplementary": "",
    "project": "",
    "demo": "",
    "abstract": "Single image depth estimation is a foundational task in computer vision and generative modeling. However prevailing depth estimation models grapple with accommodating the increasing resolutions commonplace in today's consumer cameras and devices. Existing high-resolution strategies show promise but they often face limitations ranging from error propagation to the loss of high-frequency details. We present PatchFusion a novel tile-based framework with three key components to improve the current state of the art: (1) A patch-wise fusion network that fuses a globally-consistent coarse prediction with finer inconsistent tiled predictions via high-level feature guidance (2) A Global-to-Local (G2L) module that adds vital context to the fusion network discarding the need for patch selection heuristics and (3) A Consistency-Aware Training (CAT) and Inference (CAI) approach emphasizing patch overlap consistency and thereby eradicating the necessity for post-processing. Experiments on UnrealStereo4K MVS-Synth and Middleburry 2014 demonstrate that our framework can generate high-resolution depth maps with intricate details. PatchFusion is independent of the base model for depth estimation. Notably our framework built on top of SOTA ZoeDepth brings improvements for a total of 17.3% and 29.4% in terms of the root mean squared error (RMSE) on UnrealStereo4K and MVS-Synth respectively.",
    "citation": "@InProceedings{Li_2024_CVPR,\n    author    = {Li, Zhenyu and Bhat, Shariq Farooq and Wonka, Peter},\n    title     = {PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    month     = {June},\n    year      = {2024},\n    pages     = {10016-10025}\n}"
  },
  {
    "title": "Scale-Invariant Monocular Depth Estimation via SSI Depth",
    "year": 2024,
    "venue": "ACM SIGGRAPH",
    "paper_url": "https://arxiv.org/pdf/2406.09374",
    "code": "",
    "supplementary": "https://yaksoy.github.io/papers/SIG24-SI-Depth-Supp.pdf",
    "project": "https://yaksoy.github.io/sidepth/",
    "demo": "",
    "abstract": "Existing methods for scale-invariant monocular depth estimation (SI MDE) often struggle due to the complexity of the task, and limited and non-diverse datasets, hindering generalizability in real-world scenarios. This is while shift-and-scale-invariant (SSI) depth estimation, simplifying the task and enabling training with abundant stereo datasets achieves high performance. We present a novel approach that leverages SSI inputs to enhance SI depth estimation, streamlining the networkâ€™s role and facilitating in-the-wild generalization for SI depth estimation while only using a synthetic dataset for training. Emphasizing the generation of high-resolution details, we introduce a novel sparse ordinal loss that substantially improves detail generation in SSI MDE, addressing critical limitations in existing approaches. Through in-the-wild qualitative examples and zero-shot evaluation we substantiate the practical utility of our approach in computational photography applications, showcasing its ability to generate highly detailed SI depth maps and achieve generalization in diverse scenarios.",
    "citation": "@INPROCEEDINGS{miangolehSIDepth,\nauthor={S. Mahdi H. Miangoleh and Mahesh Reddy and Ya\\u{g}{\\i}z Aksoy},\ntitle={Scale-Invariant Monocular Depth Estimation via SSI Depth},\nbooktitle={ACM SIGGRAPH},\nyear={2024},\n}"
  },
  {
    "title": "WorDepth: Variational Language Prior for Monocular Depth Estimation",
    "year": 2024,
    "venue": "CVPR",
    "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zeng_WorDepth_Variational_Language_Prior_for_Monocular_Depth_Estimation_CVPR_2024_paper.pdf",
    "code": "https://github.com/Adonis-galaxy/WorDepth",
    "supplementary": "https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zeng_WorDepth_Variational_Language_CVPR_2024_supplemental.pdf",
    "project": "",
    "demo": "",
    "abstract": "Three-dimensional (3D) reconstruction from a single image is an ill-posed problem with inherent ambiguities i.e. scale. Predicting a 3D scene from text description(s) is similarly ill-posed i.e. spatial arrangements of objects described. We investigate the question of whether two inherently ambiguous modalities can be used in conjunction to produce metric-scaled reconstructions. To test this we focus on monocular depth estimation the problem of predicting a dense depth map from a single image but with an additional text caption describing the scene. To this end we begin by encoding the text caption as a mean and standard deviation; using a variational framework we learn the distribution of the plausible metric reconstructions of 3D scenes corresponding to the text captions as a prior. To 'select' a specific reconstruction or depth map we encode the given image through a conditional sampler that samples from the latent space of the variational text encoder which is then decoded to the output depth map. Our approach is trained alternatingly between the text and image branches: in one optimization step we predict the mean and standard deviation from the text description and sample from a standard Gaussian and in the other we sample using a (image) conditional sampler. Once trained we directly predict depth from the encoded text using the conditional sampler. We demonstrate our approach on indoor (NYUv2) and outdoor (KITTI) scenarios where we show that language can consistently improve performance in both. Code: https://github.com/Adonis-galaxy/WorDepth.",
    "citation": "@InProceedings{Zeng_2024_CVPR,\n    author    = {Zeng, Ziyao and Wang, Daniel and Yang, Fengyu and Park, Hyoungseob and Soatto, Stefano and Lao, Dong and Wong, Alex},\n    title     = {WorDepth: Variational Language Prior for Monocular Depth Estimation},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    month     = {June},\n    year      = {2024},\n    pages     = {9708-9719}\n}"
  },
  {
    "title": "BinsFormer: Revisiting Adaptive Bins for Monocular Depth Estimation",
    "year": 2022,
    "venue": "IEEE TIP",
    "paper_url": "https://arxiv.org/pdf/2204.00987",
    "code": "https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox/tree/main/configs/binsformer.",
    "supplementary": "",
    "project": "",
    "demo": "",
    "abstract": "Monocular depth estimation (MDE) is a fundamental task in computer vision and has drawn increasing attention. Recently, some methods reformulate it as a classification-regression task to boost the model performance, where continuous depth is estimated via a linear combination of predicted probability distributions and discrete bins. In this paper, we present a novel framework called BinsFormer, tailored for the classification-regression-based depth estimation. It mainly focuses on two crucial components in the specific task: 1) proper generation of adaptive bins; and 2) sufficient interaction between probability distribution and bins predictions. To specify, we employ a Transformer decoder to generate bins, novelly viewing it as a direct set-to-set prediction problem. We further integrate a multi-scale decoder structure to achieve a comprehensive understanding of spatial geometry information and estimate depth maps in a coarse-to-fine manner. Moreover, an extra scene understanding query is proposed to improve the estimation accuracy, which turns out that models can implicitly learn useful information from the auxiliary environment classification task. Extensive experiments on the KITTI, NYU, and SUN RGB-D datasets demonstrate that BinsFormer surpasses state-of-the-art MDE methods with prominent margins. Code and pretrained models are made publicly available at https://github.com/zhyever/ Monocular-Depth-Estimation-Toolbox/tree/main/configs/ binsformer.",
    "citation": "@ARTICLE{10570231,\n  author={Li, Zhenyu and Wang, Xuyang and Liu, Xianming and Jiang, Junjun},\n  journal={IEEE Transactions on Image Processing}, \n  title={BinsFormer: Revisiting Adaptive Bins for Monocular Depth Estimation}, \n  year={2024},\n  volume={33},\n  number={},\n  pages={3964-3976},\n  keywords={Estimation;Transformers;Task analysis;Decoding;Probabilistic logic;Training;Computer vision;Monocular depth estimation;adaptive bins;multi-scale refinement;auxiliary task;transformer},\n  doi={10.1109/TIP.2024.3416065}\n}"
  },
  {
    "title": "AdaBins: Depth Estimation Using Adaptive Bins",
    "year": 2021,
    "venue": "CVPR",
    "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Bhat_AdaBins_Depth_Estimation_Using_Adaptive_Bins_CVPR_2021_paper.pdf",
    "code": "https://github.com/shariqfarooq123/AdaBins",
    "supplementary": "https://openaccess.thecvf.com/content/CVPR2021/supplemental/Bhat_AdaBins_Depth_Estimation_CVPR_2021_supplemental.zip",
    "project": "",
    "demo": "https://colab.research.google.com/drive/1oxHflMh6eAJS7BhvP1amHvuBSirlS5Vl?usp=sharing",
    "abstract": "We address the problem of estimating a high quality dense depth map from a single RGB input image. We start out with a baseline encoder-decoder convolutional neural network architecture and pose the question of how the global processing of information can help improve overall depth estimation. To this end, we propose a transformer-based architecture block that divides the depth range into bins whose center value is estimated adaptively per image. The final depth values are estimated as linear combinations of the bin centers. We call our new building block AdaBins. Our results show a decisive improvement over the state-of-the-art on several popular depth datasets across all metrics. We also validate the effectiveness of the proposed block with an ablation study and provide the code and corresponding pre-trained weights of the new state-of-the-art model.",
    "citation": "@InProceedings{Bhat_2021_CVPR,\n    author    = {Bhat, Shariq Farooq and Alhashim, Ibraheem and Wonka, Peter},\n    title     = {AdaBins: Depth Estimation Using Adaptive Bins},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    month     = {June},\n    year      = {2021},\n    pages     = {4009-4018}\n}"
  }
]